# Training Summary

## Overview

Complete training pipeline for a convolutional autoencoder on seismic waveform STFT spectrograms.

## What Was Created

### 1. Dataset (`stft_dataset.py`)
- **SeismicSTFTDataset**: PyTorch dataset class
- Loads mseed files from `data/filtered_waveforms/`
- Each file contains 3 traces (E, N, Z components)
- Converts each component to STFT spectrogram
- Returns 3-channel image: (3, freq_bins, time_bins)
- Supports multiple channel types: HH, HN, EH, BH
- Custom collate function for variable-sized spectrograms

**Features:**
- Configurable STFT parameters (nperseg, noverlap, nfft)
- Log scaling and normalization
- Automatic padding for batch processing
- Error handling for corrupted files

### 2. Model (`model.py`)
- **ConvAutoencoder**: 2D convolutional autoencoder
- **Encoder**: 4 convolutional layers (32â†’64â†’128â†’256 features)
- **Decoder**: 4 transpose convolutional layers (256â†’128â†’64â†’32â†’3)
- Latent space: (256, H/16, W/16)
- Output interpolation to match input size
- ~778k parameters

### 3. Training Script (`train.py`)
Comprehensive training pipeline with:
- **Data**: Train/val split, multi-worker data loading
- **Training**: Adam optimizer, MSE loss, gradient updates
- **Monitoring**: TensorBoard logging (loss curves, images)
- **Checkpointing**: Saves best model + periodic checkpoints
- **Configuration**: All hyperparameters via command-line args
- **Resuming**: Can resume from checkpoint

**Key Features:**
- Progress bars with tqdm
- Automatic checkpoint cleanup
- Configuration saving (JSON)
- GPU/CPU support with automatic fallback

### 4. Inference Script (`inference.py`)
Test trained models:
- Load checkpoint
- Visualize reconstructions (input vs output)
- Compute metrics (MSE, MAE per channel)
- Save comparison images
- Batch testing on random samples

### 5. Testing Script (`test_pipeline.py`)
Quick validation:
- Tests dataset loading
- Tests model creation
- Tests forward/backward pass
- Tests batch processing
- Verifies complete pipeline works

### 6. Documentation (`README.md`)
Complete guide with:
- Quick start examples
- All command-line options
- Model architecture details
- TensorBoard instructions
- Troubleshooting tips
- Expected output examples

## Usage Examples

### 1. Test Pipeline
```bash
python test_pipeline.py
```

### 2. Basic Training
```bash
python train.py
```

### 3. Advanced Training
```bash
python train.py \
    --channels HH HN EH \
    --batch_size 32 \
    --num_epochs 100 \
    --lr 1e-3 \
    --num_workers 8
```

### 4. Monitor Training
```bash
tensorboard --logdir logs
```

### 5. Test Trained Model
```bash
python inference.py checkpoints/<timestamp>/best_model.pt --num_samples 20
```

## Data Statistics

- **HH**: 91,429 files (most common)
- **HN**: 35,136 files
- **EH**: 4,453 files
- **BH**: 1,151 files
- **Total**: 132,169 seismic waveform files

Each file contains:
- 3 traces (E, N, Z components)
- ~7000 samples per trace
- 100 Hz sampling rate
- ~70 seconds duration

## STFT Output

Default parameters (nperseg=256, noverlap=192, nfft=256):
- **Frequency bins**: 129 (0 to Nyquist)
- **Time bins**: ~110 (depends on signal length)
- **Shape**: (3, 129, ~110)

## Training Performance

Estimated training time (single GPU):
- **HH only** (91k samples): ~2-3 hours per epoch (batch_size=16)
- **All channels** (132k samples): ~4-5 hours per epoch (batch_size=16)

With batch_size=32 and num_workers=8:
- Training: ~8-10 batches/sec
- Validation: ~10-12 batches/sec

## Model Performance

Expected reconstruction quality:
- **MSE**: 0.01-0.02 (after convergence)
- **MAE**: 0.05-0.08 (after convergence)
- Better on high SNR signals
- Learns to denoise and compress spectrograms

## Next Steps

1. **Run test pipeline**: Verify everything works
2. **Start training**: Use default settings first
3. **Monitor with TensorBoard**: Watch loss curves and reconstructions
4. **Experiment**: Try different STFT parameters, batch sizes, architectures
5. **Evaluate**: Use inference script to visualize results
6. **Iterate**: Adjust hyperparameters based on results

## Files Created

```
ML/autoencoder/
â”œâ”€â”€ model.py              # Autoencoder architecture
â”œâ”€â”€ stft_dataset.py       # Dataset class
â”œâ”€â”€ train.py              # Training script
â”œâ”€â”€ inference.py          # Inference script
â”œâ”€â”€ test_pipeline.py      # Pipeline test
â”œâ”€â”€ README.md             # Documentation
â””â”€â”€ TRAINING_SUMMARY.md   # This file
```

After training:
```
ML/autoencoder/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ <timestamp>/
â”‚       â”œâ”€â”€ best_model.pt
â”‚       â”œâ”€â”€ checkpoint_epoch_N.pt
â”‚       â””â”€â”€ config.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ <timestamp>/
â”‚       â””â”€â”€ events.out.tfevents...
â””â”€â”€ inference_results/
    â””â”€â”€ reconstruction_*.png
```

## Key Design Decisions

1. **STFT instead of raw waveforms**: More compact representation, easier to learn
2. **3-channel image format**: Natural for CNNs, preserves component relationships
3. **Log scaling + normalization**: Handles large dynamic range of seismic signals
4. **Variable-size handling**: Custom collate function pads spectrograms in batch
5. **Interpolation for size matching**: Ensures output matches input dimensions
6. **MSE loss**: Simple and effective for reconstruction tasks
7. **Periodic checkpointing**: Prevents loss of progress, enables resuming

## Success Criteria

âœ… Pipeline test passes  
âœ… Training loss decreases  
âœ… Validation loss decreases  
âœ… Reconstructions visually similar to inputs  
âœ… MSE < 0.05 after convergence  
âœ… No memory errors or crashes  
âœ… Checkpoints save correctly  
âœ… TensorBoard logs work  

The training pipeline is complete and ready to use! ðŸŽ‰
